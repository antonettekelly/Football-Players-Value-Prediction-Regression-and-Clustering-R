---
title: "Predictive Value on Football Players using Decision Tree and Random Forest Algorithms"
author: "Kelly Mae"
date: "2021"
output: openintro::lab_report
---

### LOAD LIBRARY

```{r, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
#======== LOADING REQUIRED LIBRARIES ========

library(tidyverse)
library(dplyr)
library(readxl)
library(knitr) 
library(tidyverse)
library(party)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)

```

### DATA EXPLORATION, DATA UNDERSTANDING, DATA FILTERING, AND DATA CLEANSING

```{r}
#======== DATA EXPLORATION AND UNDERSTANDING ========

#Read Excel
MyData <- read_excel("Football Players Value Prediction Dataset.xlsx")

#Observe Data Structure
str(MyData)

#Change Data to Numeric
MyData$value_eur <- as.numeric(MyData$value_eur)

#Data Visualization using Boxplot
boxplot(MyData$value_eur, main = "Boxplot FIFA 21 Player's Value", xlab = "Players", ylab = "Values", col = c("cyan"), horizontal = TRUE, outline = FALSE)

#Summarize Data
summary(MyData$value_eur)

#======== DATA FILTERING AND CLEANSING ========

#Filter Data and Change to As.Factor 
MyData$value[MyData$value_eur < 650000] <- "0"
MyData$value[MyData$value_eur >= 650000] <- "1"
MyData$value <- as.factor(MyData$value)

#Select Numerical Data
numerical <- select_if(MyData, is.numeric)

#Set new Variable for Value
value <- MyData$value

#Combine Numerical Data with Value
numerical <- cbind(numerical, value)

#Drop Data using Subset
numerical <- subset(numerical, select = c(value, pace, shooting, passing, dribbling, defending, physic))

#Remove NA Value
numdata <- na.omit(numerical
                   )
#Observe Data Structure After Data Cleansing
str(numdata)

```

### DESICION TREE
## DATA SPLITTING

```{r}
#======== CREATING DATA TRAINING AND DATA TESTING (80:20) ========

#Set Seed
id <- 51428
set.seed(id)

#Split Data
sampl <- sample(nrow(numdata), 0.8 * nrow(numdata), replace = FALSE)

#Set Training and Testing Data
training <- numdata[sampl,]
testing <- numdata[-sampl,]

#Determine Amount of Data Used
nrow(MyData)
#Overall Amount of Data = 18944.

nrow(training)
#Amount of Training Data = 13488.

nrow(testing)
#Amount of Testing Data = 3373.

```

## CTREE MODEL

```{r fig, fig.height = 35, fig.width = 60}
#======== IMPLEMENTING CTREE MODEL ========

#Fit Model using Numerical Data
numdata_ctree <- ctree(value ~ ., data = numdata)
plot(numdata_ctree, type = "simple")
print(numdata_ctree)

#Fit Model using Training Data
train_ctree <- ctree(value ~ ., data = training)
plot(train_ctree, type = "simple")
print(train_ctree)

#Model Prediction using Testing Data
predict_ctree <- predict(train_ctree, testing, type = "response")
table_ctree <- table(predict_ctree, testing$value)


#======== Confusion Matrix ========

#Confusion Matrix
confusionMatrix(table_ctree)

```

## RPART MODEL

```{r}
#======== IMPLEMENTING RPART MODEL ========

#Fit Model using Numerical Data
numdata_rpart <- rpart(value ~ ., numdata, cp = .05)
rpart.plot(numdata_rpart, box.palette = "RdBu", shadow.col= "gray", nn = TRUE)
print(numdata_rpart)

#Fit Model using Training Data
train_rpart <- rpart(value ~ ., training, cp = .05)
rpart.plot(train_rpart, box.palette = "RdBu", shadow.col= "gray", nn = TRUE)
print(train_rpart)

#Model Prediction using Testing Data
predict_rpart <- predict(train_rpart, testing, type = "class")
table_rpart <- table(predict_rpart, testing$value)

#======== Confusion Matrix ========

#Confusion Matrix
confusionMatrix(table_rpart)

```

### CONCLUSION FOR DECISION TREE ALGORITHM

Based on decision tree implementations that have been done, there are 2 results of decision tree models which are Party and Rpart models that show high accuracy with a slight difference between each accuracy. Party model has reached 86.63% in accuracy, while Rpart model has shown 84.49% in accuracy. As the result, Party model is the model with the highest level of accuracy so that this model can be used in the current research with Random Forest Algorithm.


### RANDOM FOREST

```{r}
#======== IMPLEMENTING RANDOM FOREST ALGORITHM ========

#Fit Model using Numerical Data
rf <- random_forest <- randomForest(value ~ ., data = numdata)
print(rf) 
plot(rf, main = deparse(substitute(rf)))

#Fit Model using Training Data
random_forest <- randomForest(value ~ ., data = training)
print(random_forest) 
plot(random_forest, main = "Random Forest Error Rate")

#Model Prediction using Testing Data
predict_rf <- predict(random_forest, testing, type = "response")
table_rf <- table(predict_rf, testing$value)

#======== Confusion Matrix ========

#Confusion Matrix
confusionMatrix(table_rf)

```

### CONCLUSION

By using training and testing data, obtained the random forest algorithm model with higher level of accuracy that reaches 88.11%, than decision tree model algorithm which holds 86.63%. As a result, the random forest algorithm can be used to predict in upcoming time.